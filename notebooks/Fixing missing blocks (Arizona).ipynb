{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing missing blocks is a known issue between one census and another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Assuming we are in the notebooks directory, we need to move one up:\n",
    "%cd ..\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import us\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from secret import *\n",
    "from default_values import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = u'arizona'\n",
    "state_fips = us.states.lookup(state).fips\n",
    "state_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "if True:\n",
    "    print 'Attempting to load predicted population by block for:', state\n",
    "#     df = gpd.read_file(blocks_geojson_filename)\n",
    "# except IOError:\n",
    "    print '\\tNot found on disk...'\n",
    "    print '\\tDownloading and processing blocks and block groups for:', state\n",
    "    # Open 2010 blocks for state (the population):\n",
    "    print '\\tOpening 2010 blocks shapefile for:', state\n",
    "    blocks_2010_shapefile_name = blocks_2010_shapefile_dir + 'tabblock2010_' +\\\n",
    "        state_fips + '_pophu.shp'\n",
    "    blocks_df = gpd.read_file(blocks_2010_shapefile_name)\n",
    "    blocks_df['BLOCKID10'] = blocks_df['BLOCKID10'].apply(str)\n",
    "    def get_group_GEOID(blockGEOID):\n",
    "        return blockGEOID[0:-3]\n",
    "    blocks_df['Block Group GEOID'] = blocks_df['BLOCKID10'].apply(get_group_GEOID)\n",
    "    assert (blocks_df['Block Group GEOID'].apply(len)).unique() == [12]\n",
    "    # The filename for the whole state with block groups (without population):\n",
    "    print '\\tOpening ACS5 block groups shapefile for:', state\n",
    "    block_groups_shapefile_name = acs5_shapefile_dir + 'tl_' + year + '_' +\\\n",
    "        state_fips + '_bg.shp'\n",
    "    block_groups_df = gpd.read_file(block_groups_shapefile_name)\n",
    "    # We will add to this the population per block group from the API.\n",
    "    # Check GEOIDs are of length 12 because they have to be by definition,\n",
    "    # see: https://www.census.gov/geo/reference/geoidentifiers.html\n",
    "    assert (block_groups_df['GEOID'].apply(len)).unique() == [12]\n",
    "  \n",
    "    \n",
    "    \n",
    "    print set(block_groups_df['GEOID'].unique()).symmetric_difference(set(\n",
    "        blocks_df['Block Group GEOID'].unique()))\n",
    "        # ['060378002043', '060379304011', u'060371370002', u'060371370001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff = list(set(block_groups_df['GEOID'].unique()).symmetric_difference(set(\n",
    "         blocks_df['Block Group GEOID'].unique())))\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in diff:\n",
    "    if d in blocks_df['Block Group GEOID'].values:\n",
    "        print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These are the block groups that get lost:\n",
    "for i, d in enumerate(diff):\n",
    "#     print i\n",
    "#     d = str(d)\n",
    "#     print block_groups_df[block_groups_df['GEOID'].astype(str).str.contains(d)]\n",
    "    try:\n",
    "        lost_block_groups = pd.concat([lost_block_groups,\n",
    "                                      block_groups_df[block_groups_df['GEOID'].astype(str).str.contains(d)]])\n",
    "    except NameError:\n",
    "        lost_block_groups = block_groups_df[block_groups_df['GEOID'].astype(str).str.contains(d)]\n",
    "lost_block_groups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lost_block_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    print len(set(block_groups_df['GEOID'].unique()).difference(set(\n",
    "        blocks_df['Block Group GEOID'].unique())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print len(set(\n",
    "         blocks_df['Block Group GEOID'].unique()).difference(set(block_groups_df['GEOID'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print blocks_df[blocks_df['Block Group GEOID'] == '040194705002']['POP10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print blocks_df[blocks_df['Block Group GEOID'] == '060371370001']['POP10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        # Code to get the total estimated population from the census files:\n",
    "        api_population = 'B01003_001E'  # http://api.census.gov/data/2015/acs5/variables.html\n",
    "        # NB: this is not the same code in the shapefiles for each block from the 2010\n",
    "        # census, which instead uses 'POP10'.\n",
    "        # Get all the counties' IDs so we can d/l each county's population per\n",
    "        # block group:\n",
    "        def block_group_to_county_geoid(geoid):\n",
    "            return geoid[2:5]\n",
    "        block_groups_df['County'] = block_groups_df['GEOID'].apply(block_group_to_county_geoid)\n",
    "        block_groups_for_state = acs5_population_dir + state + '.json'\n",
    "        try:\n",
    "            print '\\tAttempting to load ACS5 population for block groups by county for:', state\n",
    "            with open(block_groups_for_state, 'r') as infile:\n",
    "                population_df = pd.read_json(infile)\n",
    "        except IOError:\n",
    "            print '\\t\\tNot found on disk...'\n",
    "            counties = block_groups_df['County'].unique()\n",
    "            for county in counties:\n",
    "                print '\\t\\tDownloading block groups for county:', county\n",
    "                url = 'http://api.census.gov/data/' + year + '/acs5?get=NAME,' +\\\n",
    "                    api_population + '&for=block+group:*&in=state:' + \\\n",
    "                    state_fips + '+county:' + county + '&key=' + apikey\n",
    "                # Make a get request to get the population of each county per block\n",
    "                # group:\n",
    "                response = requests.get(url)\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except ValueError:  # includes simplejson.decoder.JSONDecodeError\n",
    "                    # This is what happens when the server times out etc.\n",
    "                    print 'Decoding JSON has failed. Server response:',\\\n",
    "                        response.status_code\n",
    "                    exit()\n",
    "                # Merge the df to get one huge df for the whole state which contains\n",
    "                # all the group blocks and their populations:\n",
    "                columns = data.pop(0) # columns as list and leaves data intact\n",
    "                try:\n",
    "                    # Try appending to previous population_df:\n",
    "                    population_df = population_df.append(\n",
    "                        pd.DataFrame(data, columns=columns),\n",
    "                        ignore_index=True)\n",
    "                except NameError:\n",
    "                    # There is no previous, so create it:\n",
    "                    population_df = pd.DataFrame(data, columns=columns)\n",
    "            population_df.to_json(block_groups_for_state)\n",
    "        print '\\tDone!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def create_geoid(row):\n",
    "        # GEOID Structure is defined as STATE + COUNTY + TRACT + BLOCK GROUP =\n",
    "        # 2 + 3 + 6 + 1 = 12\n",
    "        # see: https://www.census.gov/geo/reference/geoidentifiers.html\n",
    "        STATE = str(row['state']).zfill(2)  # zero padding to conform to GEOID\n",
    "        COUNTY = str(row['county']).zfill(3)\n",
    "        TRACT = str(row['tract']).zfill(6)\n",
    "        BLOCK_GROUP = str(row['block group'])\n",
    "        assert len(STATE + COUNTY + TRACT + BLOCK_GROUP) == 12\n",
    "        return STATE + COUNTY + TRACT + BLOCK_GROUP\n",
    "    population_df['GEOID'] = population_df.apply(create_geoid, axis=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_groups_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    block_groups_df = block_groups_df.merge(population_df, on='GEOID')\n",
    "    assert block_groups_df['GEOID'].shape == population_df['GEOID'].shape\n",
    "#     del population_df\n",
    "    # Get rid of other columns, we are just interested in below for each block\n",
    "    # group:\n",
    "    block_groups_df = block_groups_df.loc[:,['GEOID',api_population]]\n",
    "    # We need to calculate the proportion of the populalion of 2010 block group\n",
    "    # that a constituent 2010 block has:\n",
    "    # a. get the total population for each block group from 2010 and associate\n",
    "    # each 2010 block with its total:\n",
    "    blocks_df['POP10'] = pd.to_numeric(blocks_df['POP10'])\n",
    "    blocks_df = blocks_df.join(blocks_df.groupby('Block Group GEOID')[\n",
    "                               'POP10'].sum(), on='Block Group GEOID',\n",
    "                                rsuffix=' for 2010 Block Group')\n",
    "    blocks_df.rename(columns={'POP10 for 2010 Block Group':\\\n",
    "                              'Population for 2010 Block Group'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blocks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# b. get the ratio of block population to respective block group\n",
    "# population:\n",
    "def calculate_ratio(row):\n",
    "    try:\n",
    "        return row['POP10'] / row['Population for 2010 Block Group']\n",
    "    except:\n",
    "        return row['Population for 2010 Block Group']\n",
    "blocks_df['Population Ratio'] = blocks_df.apply(calculate_ratio, axis=1)\n",
    "# c. get the population for the 2015 ACS5 block groups and associate them\n",
    "# with their respective blocks:\n",
    "blocks_df = pd.merge(blocks_df, block_groups_df,\n",
    "                     left_on = 'Block Group GEOID',\n",
    "                     right_on = 'GEOID')\n",
    "blocks_df.rename(columns={api_population:\\\n",
    "                    'Population for ' + year + ' Block Group'},\n",
    "                 inplace=True)\n",
    "assert (blocks_df['GEOID'] == blocks_df['Block Group GEOID']).all()\n",
    "del blocks_df['GEOID']\n",
    "blocks_df['Population for ' + year + ' Block Group'] =\\\n",
    "    pd.to_numeric(blocks_df['Population for ' + year + ' Block Group'])\n",
    "assert block_groups_df[api_population].sum() > blocks_df['POP10'].sum()\n",
    "# assert blocks_df['Population for 2010 Block Group'].sum() <\\\n",
    "    # blocks_df['Population for ' + year + ' Block Group'].sum()\n",
    "#     del block_groups_df # we do not need this anymore\n",
    "# d. calculate the predicted population for the block based on the above:\n",
    "blocks_df[population] = blocks_df['Population Ratio'] *\\\n",
    "    blocks_df['Population for ' + year + ' Block Group']\n",
    "# Calculate the centroids for each block required for clustering:\n",
    "def get_x(p): return p.x\n",
    "def get_y(p): return p.y\n",
    "blocks_df['Centroid Longitude'] = blocks_df['geometry'].centroid.apply(get_x)\n",
    "blocks_df['Centroid Latitude'] = blocks_df['geometry'].centroid.apply(get_y)\n",
    "# Find which congressional district each block group for this state belongs\n",
    "#  to. This is done using the block assignments files downloaded from:\n",
    "#  https://www.census.gov/geo/maps-data/data/baf.html\n",
    "# print '\\tCalculating congressional districts for:', state\n",
    "block_assignments_df = pd.read_csv(block_assignments_dir +\\\n",
    "    'National_CD115.txt', dtype={\"BLOCKID\": str, \"CD115\": str})\n",
    "block_assignments_df.rename(columns={'BLOCKID': 'GEOID'}, inplace=True)\n",
    "blocks_df.rename(columns={'BLOCKID10': 'GEOID'}, inplace=True)\n",
    "blocks_df = blocks_df.merge(block_assignments_df, how='left',\n",
    "    indicator=True)\n",
    "assert blocks_df['_merge'].unique() == ['both']\n",
    "del blocks_df['_merge']\n",
    "del block_assignments_df\n",
    "# We are done now, check that columns are what we expect before we remove\n",
    "# useless ones:\n",
    "assert list(blocks_df) ==\\\n",
    "    [u'BLOCKCE', 'GEOID', u'COUNTYFP10', u'HOUSING10', u'PARTFLG', u'POP10',\n",
    "     u'STATEFP10', u'TRACTCE10', 'geometry', 'Block Group GEOID',\n",
    "     'Population for 2010 Block Group', 'Population Ratio',\n",
    "     'Population for 2015 Block Group', 'Predicted 2015 Population',\n",
    "     'Centroid Longitude', 'Centroid Latitude', 'CD115']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blocks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lost_block_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Get rid of colums we do not need:\n",
    "blocks_df = blocks_df.loc[:,['GEOID', 'CD115', 'geometry',\n",
    "                                 'Predicted 2015 Population',\n",
    "                                 'Centroid Longitude', 'Centroid Latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blocks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Give columns better names:\n",
    "blocks_df.rename(columns={'CD115': 'Congressional District',\n",
    "                          'Centroid Longitude': 'Centroid Longitude',\n",
    "                          'Centroid Latitude': 'Centroid Latitude'},\n",
    "                  inplace=True)\n",
    "blocks_df['Congressional District'] = blocks_df['Congressional District'].apply(str)\n",
    "blocks_df['Centroid Longitude'] = blocks_df['Centroid Longitude'].apply(float)\n",
    "blocks_df['Centroid Latitude'] = blocks_df['Centroid Latitude'].apply(float)\n",
    "blocks_df['Predicted 2015 Population'] = blocks_df['Predicted 2015 Population'].apply(float)\n",
    "# We are finally done, save the file!\n",
    "print 'Saving file...'\n",
    "blocks_df = gpd.GeoDataFrame(blocks_df)\n",
    "with open(blocks_geojson_filename, 'w') as outfile:\n",
    "    outfile.write(blocks_df.to_json())\n",
    "# In the rest of this code the block-wise dataframe with predicted\n",
    "# populations is called just df:\n",
    "df = blocks_df\n",
    "del blocks_df\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# del lost_block_groups['ALAND']\n",
    "# del lost_block_groups['AWATER']\n",
    "# del lost_block_groups['BLKGRPCE']\n",
    "\n",
    "population_df.head()\n",
    "lost_block_groups = lost_block_groups.merge(population_df, on='GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lost_block_groups.head()\n",
    "lost_block_groups['Centroid Longitude'] = lost_block_groups['geometry'].centroid.apply(get_x)\n",
    "    \n",
    "lost_block_groups['Centroid Latitude'] = lost_block_groups['geometry'].centroid.apply(get_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lost_block_groups.rename(columns={'B01003_001E': 'Predicted 2015 Population'}, inplace=True)\n",
    "\n",
    "lost_block_groups = lost_block_groups[['GEOID', 'geometry', 'Predicted 2015 Population', 'Centroid Longitude', 'Centroid Latitude']]\n",
    "lost_block_groups.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_groups_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usa = gpd.read_file(usa_shapefile_path)\n",
    "cong_dist = usa[usa['STATEFP'].apply(\n",
    "    int) == int(state_fips)]\n",
    "del usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cong_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_bg_cong_dist = pd.DataFrame(columns=[['GEOID', 'Congressional District']])\n",
    "missing_bg_cong_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geoid = []\n",
    "cd = []\n",
    "for bg_index, bg_row in lost_block_groups.iterrows():\n",
    "    for dist_index, dist_row in cong_dist.iterrows():\n",
    "        bg_area =  bg_row['geometry'].area\n",
    "        intersection_area = bg_row['geometry'].intersection(dist_row['geometry']).area\n",
    "        if bg_area - intersection_area < 1e-10:\n",
    "#             print bg_index, dist_row['CD115FP']\n",
    "#             print intersection_area, bg_area\n",
    "            geoid.append(bg_row['GEOID'])\n",
    "            cd.append(dist_row['CD115FP'])\n",
    "        \n",
    "geoid, cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_bg_cong_dist['GEOID'] = geoid\n",
    "missing_bg_cong_dist['Congressional District'] = cd\n",
    "missing_bg_cong_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lost_block_groups = lost_block_groups.merge(missing_bg_cong_dist, on = 'GEOID')\n",
    "# del lost_block_groups['Congressional District_x']\n",
    "# del lost_block_groups['Congressional District_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lost_block_groups.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "df = df.append(lost_block_groups, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dissolve(by='Congressional District').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(prediction_geojson_dir+'AZ.geojson', 'w') as outfile:\n",
    "    outfile.write(df.to_json())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
